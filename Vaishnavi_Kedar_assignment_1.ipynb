{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOr7RqRovrGGiam0oLTL+Rv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaishnavi-kedar/demo1/blob/main/Vaishnavi_Kedar_assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8B82bUY3RPSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objective:Implement a simple feedforward neural network from scratch in Python without using any in-built deep learning libraries. This implementation will focus on basic components like forward pass, backward propagation(back-propagation), and training using gradient descent.\n",
        "\n",
        "Submitted By:\n",
        "\n",
        "Name : Vaishnavi Kedar\n",
        "\n",
        "Roll No : 18\n",
        "\n",
        "PRN No : 202201040045\n",
        "\n",
        "Detailed Comments Explanation:\n",
        "\n",
        "Sigmoid and Derivative:\n",
        "\n",
        "sigmoid(x) computes the output of the sigmoid activation function. It maps any real-valued input into a value between 0 and 1. sigmoid_derivative(x) is the derivative of the sigmoid function, which is used during backpropagation to calculate gradients. Neural Network Class:\n",
        "\n",
        "The NeuralNetwork class contains the methods to initialize the network, perform the forward and backward passes, and train the network. Forward Pass:\n",
        "\n",
        "The forward pass computes the activations of the neurons in the network. The input is passed through the layers, with the weights and biases applied, followed by the activation function (sigmoid) to compute the output. Backward Pass (Backpropagation):\n",
        "\n",
        "During the backward pass, the weights are updated by calculating the error between the predicted output and the true output. The gradients are calculated using the derivative of the sigmoid function, and the weights and biases are updated using gradient descent with a specified learning rate. Training Method:\n",
        "\n",
        "The network is trained by performing multiple epochs, where each epoch involves a forward pass followed by a backward pass. Every 1000 epochs, the loss (mean squared error) is printed to track the network's progress in learning. Main Program:\n",
        "\n",
        "The main program defines a simple XOR dataset, where the inputs are 0 and 1 combinations, and the output is their XOR result. The network is created with 2 input neurons, 4 hidden neurons, and 1 output neuron. The network is trained on the XOR data for 10,000 epochs with a learning rate of 0.1. Output:\n",
        "\n",
        "After training, the network is tested on the same XOR inputs, and the predictions are printed."
      ],
      "metadata": {
        "id": "Nj5MiKvJRRy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colab Link : https://colab.research.google.com/drive/12gj_mi7MtBzAni5qSEf5irmdAoCN5Yeo?usp=sharing                        \n",
        "\n",
        "GitHub Link : https://github.com/vaishnavi-kedar/demo1/blob/c9bc88791a305dd4a9e29659f09f056e0318c185/Lab_Assignmnent_3.ipynb"
      ],
      "metadata": {
        "id": "f81xvrMtAbRf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbgvlKm5Jstn",
        "outputId": "a8f7073e-4ef2-4965-f58f-950bf8372c1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the neural network...\n",
            "Epoch 0 - Loss: 0.09592431476548918\n",
            "Epoch 1000 - Loss: 0.002612043693978449\n",
            "Epoch 2000 - Loss: 0.0012166072307213845\n",
            "Epoch 3000 - Loss: 0.0007813300824130758\n",
            "Epoch 4000 - Loss: 0.000571928398918864\n",
            "Epoch 5000 - Loss: 0.00044956724604263136\n",
            "Epoch 6000 - Loss: 0.0003695904812269742\n",
            "Epoch 7000 - Loss: 0.00031334846551726153\n",
            "Epoch 8000 - Loss: 0.0002717020303729652\n",
            "Epoch 9000 - Loss: 0.00023965609991941393\n",
            "\n",
            "Prediction after training:\n",
            "[[0.98536257]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid Activation Function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Derivative of Sigmoid Activation Function\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Neural Network Class Definition\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output):\n",
        "        \"\"\"\n",
        "        Initialize the neural network with weights and biases provided manually.\n",
        "        \"\"\"\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Initialize weights and biases from the provided values\n",
        "        self.weights_input_hidden = np.array(weights_input_hidden)\n",
        "        self.bias_hidden = np.array(bias_hidden)\n",
        "        self.weights_hidden_output = np.array(weights_hidden_output)\n",
        "        self.bias_output = np.array(bias_output, dtype=float) # Ensure bias_output is float64\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.input_layer = X\n",
        "        self.hidden_layer_input = np.dot(self.input_layer, self.weights_input_hidden) + self.bias_hidden\n",
        "        self.hidden_layer_output = sigmoid(self.hidden_layer_input)\n",
        "        self.output_layer_input = np.dot(self.hidden_layer_output, self.weights_hidden_output) + self.bias_output\n",
        "        self.output_layer_output = sigmoid(self.output_layer_input)\n",
        "        return self.output_layer_output\n",
        "\n",
        "    def backward(self, X, y, learning_rate):\n",
        "        error_output = y - self.output_layer_output\n",
        "        output_layer_delta = error_output * sigmoid_derivative(self.output_layer_output)\n",
        "        error_hidden = output_layer_delta.dot(self.weights_hidden_output.T)\n",
        "        hidden_layer_delta = error_hidden * sigmoid_derivative(self.hidden_layer_output)\n",
        "\n",
        "        self.weights_hidden_output += self.hidden_layer_output.T.dot(output_layer_delta) * learning_rate\n",
        "        self.bias_output += np.sum(output_layer_delta, axis=0, keepdims=True) * learning_rate\n",
        "        self.weights_input_hidden += X.T.dot(hidden_layer_delta) * learning_rate\n",
        "        self.bias_hidden += np.sum(hidden_layer_delta, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        for epoch in range(epochs):\n",
        "            self.forward(X)\n",
        "            self.backward(X, y, learning_rate)\n",
        "            if epoch % 1000 == 0:\n",
        "                loss = np.mean(np.square(y - self.output_layer_output))\n",
        "                print(f\"Epoch {epoch} - Loss: {loss}\")\n",
        "\n",
        "# Main Program\n",
        "if __name__ == \"__main__\":\n",
        "    # Define weights and biases from the diagram\n",
        "    weights_input_hidden = [\n",
        "        [0.1, 0.4],  # Weights from x1 to H3 and H4\n",
        "        [0.8, 0.6]   # Weights from x2 to H3 and H4\n",
        "    ]\n",
        "    bias_hidden = [[0.0, 0.0]]  # Initialize bias_hidden with float values\n",
        "\n",
        "    weights_hidden_output = [\n",
        "        [0.3],  # Weight from H3 to Output\n",
        "        [0.9]   # Weight from H4 to Output\n",
        "    ]\n",
        "    bias_output = [[0.0]]  # Initialize bias_output with a float value\n",
        "\n",
        "    # XOR Dataset (for input testing)\n",
        "    X = np.array([[0.35, 0.9]])  # Input data from the diagram\n",
        "    y = np.array([[1]])  # Expected output from the diagram (example)\n",
        "\n",
        "    # Initialize and train the neural network\n",
        "    nn = NeuralNetwork(\n",
        "        input_size=2,\n",
        "        hidden_size=2,\n",
        "        output_size=1,\n",
        "        weights_input_hidden=weights_input_hidden,\n",
        "        bias_hidden=bias_hidden,\n",
        "        weights_hidden_output=weights_hidden_output,\n",
        "        bias_output=bias_output\n",
        "    )\n",
        "\n",
        "    # Training for demonstration\n",
        "    print(\"Training the neural network...\")\n",
        "    nn.train(X, y, epochs=10000, learning_rate=0.1)\n",
        "\n",
        "    # Predictions after training\n",
        "    print(\"\\nPrediction after training:\")\n",
        "    prediction = nn.forward(X)\n",
        "    print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Declaration**\n",
        "\n",
        "I, Vaishnavi Kedar, confirm that the work submitted in this assignment is my own and has been completed following academic integrity guidelines. The code is uploaded on my GitHub repository account, and the repository link is provided below:\n",
        "\n",
        "GitHub Repository Link: https://github.com/vaishnavi-kedar/demo1/blob/main/Lab_Assignment4.ipynb\n",
        "\n",
        "Signature: Vaishnavi Kedar"
      ],
      "metadata": {
        "id": "FTm39yYCA27Q"
      }
    }
  ]
}